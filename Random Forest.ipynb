{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    857409.000000\n",
       "mean          1.731272\n",
       "std           7.080017\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max         273.000000\n",
       "Name: TotalTimeStopped_p20, dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "#iris = datasets.load_iris()\n",
    "features=pd.read_csv(\"train_data.csv\")\n",
    "#features=pd.DataFrame({\n",
    " #   'sepal length':iris.data[:,0],\n",
    "  #  'sepal width':iris.data[:,1],\n",
    "   # 'petal length':iris.data[:,2],\n",
    "    #'petal width':iris.data[:,3],\n",
    "    #'species':iris.target\n",
    "#})\n",
    "features[\"TotalTimeStopped_p20\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2=features\n",
    "colnames=list(features.columns)\n",
    "# Variable: SameEntryTRUE        Importance: 0.0\n",
    "# Variable: CityBoston           Importance: 0.0\n",
    "# Variable: CityChicago          Importance: 0.0\n",
    "# Variable: CityPhiladelphia     Importance: 0.0\n",
    "# Variable: RushHour:SameEntryTRUE Importance: 0.0\n",
    "# Variable: Weekend:CityBoston   Importance: 0.0\n",
    "# Variable: Weekend:CityChicago  Importance: 0.0\n",
    "# Variable: RushHour:CityBoston  Importance: 0.0\n",
    "# Variable: RushHour:CityChicago Importance: 0.0\n",
    "# Variable: RushHour:CityPhiladelphia Importance: 0.0\n",
    "# Variable: ExitHeadingNE:RushHour Importance: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Latitude',\n",
       " 'Longitude',\n",
       " 'EntryHeadingN',\n",
       " 'EntryHeadingNE',\n",
       " 'EntryHeadingNW',\n",
       " 'EntryHeadingS',\n",
       " 'EntryHeadingSE',\n",
       " 'EntryHeadingSW',\n",
       " 'EntryHeadingW',\n",
       " 'ExitHeadingN',\n",
       " 'ExitHeadingNE',\n",
       " 'ExitHeadingNW',\n",
       " 'ExitHeadingS',\n",
       " 'ExitHeadingSE',\n",
       " 'ExitHeadingSW',\n",
       " 'ExitHeadingW',\n",
       " 'RushHour',\n",
       " 'Weekend',\n",
       " 'RushMonth',\n",
       " 'SameStreetTRUE',\n",
       " 'RushHour:Weekend',\n",
       " 'Weekend:CityBoston',\n",
       " 'Weekend:CityChicago',\n",
       " 'Weekend:CityPhiladelphia',\n",
       " 'RushMonth:CityBoston',\n",
       " 'RushMonth:CityChicago',\n",
       " 'RushMonth:CityPhiladelphia',\n",
       " 'RushMonth:SameEntryTRUE',\n",
       " 'ExitHeadingN:RushHour',\n",
       " 'ExitHeadingNE:RushHour',\n",
       " 'ExitHeadingNW:RushHour',\n",
       " 'ExitHeadingS:RushHour',\n",
       " 'ExitHeadingSE:RushHour',\n",
       " 'ExitHeadingSW:RushHour',\n",
       " 'ExitHeadingW:RushHour']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames_2=colnames[:20]+[colnames[39]]+colnames[41:44]+colnames[47:]\n",
    "colnames_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(features['TotalTimeStopped_p20'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "#list_of_features=colnames[:24]+colnames[39:]\n",
    "#features= features[list_of_features]\n",
    "# Saving feature names for later use\n",
    "# Convert to numpy array\n",
    "#features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857409, 35)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Improving\n",
    "features_2= features_2[colnames_2]\n",
    "features_2=np.array(features_2)\n",
    "features_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving\n",
    "train_features2, test_features2, train_labels2, test_labels2 = train_test_split(features_2, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (685927, 35)\n",
      "Training Labels Shape: (685927,)\n",
      "Testing Features Shape: (171482, 35)\n",
      "Testing Labels Shape: (171482,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features2.shape)\n",
    "print('Training Labels Shape:', train_labels2.shape)\n",
    "print('Testing Features Shape:', test_features2.shape)\n",
    "print('Testing Labels Shape:', test_labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline predictions are the historical averages\n",
    "#baseline_preds \n",
    "# Baseline errors, and display average baseline error\n",
    "#baseline_errors = abs(baseline_preds - test_labels)\n",
    "#print('Average baseline error: ', round(np.mean(baseline_errors), 2))\n",
    "#Average baseline error:  5.06 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "# Train the model on training data\n",
    "# Cconvert \n",
    "rf.fit(train_features2, train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features2)\n",
    "#Calculate the absolute errors\n",
    "# Print out the mean absolute error (mae)\n",
    "#Mean Absolute Error: 3.83 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.2876516334260044\n"
     ]
    }
   ],
   "source": [
    "#Calculate the absolute errors\n",
    "errors = sum((predictions - test_labels)**2)\n",
    "SST=sum((test_labels-np.mean(test_labels))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2630202413216435"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores=cross_val_score(rf,train_features,train_labels,cv=5)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "#scores=cross_val_score(rf,train_features,train_labels,cv=5)\n",
    "#np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools needed for visualization\n",
    "#from sklearn.tree import export_graphviz\n",
    "#import pydot\n",
    "# Pull out one tree from the forest\n",
    "#tree = rf.estimators_[5]\n",
    "# Import tools needed for visualization\n",
    "#from sklearn.tree import export_graphviz\n",
    "#import pydot\n",
    "# Pull out one tree from the forest\n",
    "#tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "#export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "# Use dot file to create a graph\n",
    "#(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file\n",
    "#graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit depth of tree to 3 levels\n",
    "#rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\n",
    "#rf_small.fit(train_features, train_labels)\n",
    "# Extract the small tree\n",
    "#tree_small = rf_small.estimators_[5]\n",
    "# Save the tree as a png image\n",
    "#export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "#(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "#graph.write_png('small_tree.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: Latitude             Importance: 0.26\n",
      "Variable: Longitude            Importance: 0.25\n",
      "Variable: EntryHeadingW        Importance: 0.04\n",
      "Variable: RushMonth            Importance: 0.04\n",
      "Variable: EntryHeadingN        Importance: 0.03\n",
      "Variable: EntryHeadingS        Importance: 0.03\n",
      "Variable: SameStreetTRUE       Importance: 0.03\n",
      "Variable: EntryHeadingSE       Importance: 0.02\n",
      "Variable: ExitHeadingN         Importance: 0.02\n",
      "Variable: ExitHeadingS         Importance: 0.02\n",
      "Variable: ExitHeadingW         Importance: 0.02\n",
      "Variable: Weekend              Importance: 0.02\n",
      "Variable: RushHour:Weekend     Importance: 0.02\n",
      "Variable: RushMonth:SameEntryTRUE Importance: 0.02\n",
      "Variable: ExitHeadingS:RushHour Importance: 0.02\n",
      "Variable: ExitHeadingW:RushHour Importance: 0.02\n",
      "Variable: EntryHeadingNE       Importance: 0.01\n",
      "Variable: EntryHeadingNW       Importance: 0.01\n",
      "Variable: EntryHeadingSW       Importance: 0.01\n",
      "Variable: ExitHeadingNE        Importance: 0.01\n",
      "Variable: ExitHeadingNW        Importance: 0.01\n",
      "Variable: ExitHeadingSE        Importance: 0.01\n",
      "Variable: ExitHeadingSW        Importance: 0.01\n",
      "Variable: RushHour             Importance: 0.01\n",
      "Variable: Weekend:CityPhiladelphia Importance: 0.01\n",
      "Variable: RushMonth:CityBoston Importance: 0.01\n",
      "Variable: RushMonth:CityChicago Importance: 0.01\n",
      "Variable: RushMonth:CityPhiladelphia Importance: 0.01\n",
      "Variable: ExitHeadingN:RushHour Importance: 0.01\n",
      "Variable: ExitHeadingNE:RushHour Importance: 0.01\n",
      "Variable: ExitHeadingNW:RushHour Importance: 0.01\n",
      "Variable: ExitHeadingSE:RushHour Importance: 0.01\n",
      "Variable: ExitHeadingSW:RushHour Importance: 0.01\n",
      "Variable: Weekend:CityBoston   Importance: 0.0\n",
      "Variable: Weekend:CityChicago  Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(colnames_2, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.00008360e+00, 4.26355925e+00, 0.00000000e+00, 2.90914588e+00,\n",
       "       4.08878104e+00, 1.39150836e+01, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.29303075e+00, 6.00000000e-03, 0.00000000e+00,\n",
       "       1.09637767e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.64442902e+00, 9.00000000e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.06672200e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.17545999e+00, 1.09657576e+00])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second \n",
    "labels_150 = np.array(features['TotalTimeStopped_p50'])\n",
    "train_features_150, test_features_150, train_labels_150, test_labels_150 = train_test_split(features_2, labels_150, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.544259395715575\n"
     ]
    }
   ],
   "source": [
    "rf_150 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "rf_150.fit(train_features_150, train_labels_150)\n",
    "predictions_150 = rf_150.predict(test_features_150)\n",
    "errors = sum((predictions_150 - test_labels_150)**2)\n",
    "SST=sum((test_labels_150-np.mean(test_labels_150))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_180 = np.array(features['TotalTimeStopped_p80'])\n",
    "train_features_180, test_features_180, train_labels_180, test_labels_180 = train_test_split(features_2, labels_180, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.5932061038850454\n"
     ]
    }
   ],
   "source": [
    "## Third\n",
    "rf_180 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "rf_180.fit(train_features_180, train_labels_180)\n",
    "predictions_180 = rf_180.predict(test_features_180)\n",
    "errors = sum((predictions_180 - test_labels_180)**2)\n",
    "SST=sum((test_labels_180-np.mean(test_labels_180))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_220 = np.array(features['DistanceToFirstStop_p20'])\n",
    "train_features_220, test_features_220, train_labels_220, test_labels_220 = train_test_split(features_2, labels_220, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.23197165115481067\n"
     ]
    }
   ],
   "source": [
    "## First\n",
    "rf_220.fit(train_features_220, train_labels_220)\n",
    "predictions_220 = rf_220.predict(test_features_220)\n",
    "errors = sum((predictions_220 - test_labels_220)**2)\n",
    "SST=sum((test_labels_220-np.mean(test_labels_220))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_250 = np.array(features['DistanceToFirstStop_p50'])\n",
    "train_features_250, test_features_250, train_labels_250, test_labels_250 = train_test_split(features_2, labels_250, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.4901068461085174\n"
     ]
    }
   ],
   "source": [
    "## Second\n",
    "rf_250 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "rf_250.fit(train_features_250, train_labels_250)\n",
    "predictions_250 = rf_250.predict(test_features_250)\n",
    "errors = sum((predictions_250 - test_labels_250)**2)\n",
    "SST=sum((test_labels_250-np.mean(test_labels_250))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_280 = np.array(features['DistanceToFirstStop_p80'])\n",
    "train_features_280, test_features_280, train_labels_280, test_labels_280 = train_test_split(features_2, labels_280, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6914584752665017\n"
     ]
    }
   ],
   "source": [
    "## Third\n",
    "rf_280 = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "rf_280.fit(train_features_280, train_labels_280)\n",
    "predictions_280 = rf_280.predict(test_features_280)\n",
    "errors = sum((predictions_280 - test_labels_280)**2)\n",
    "SST=sum((test_labels_280-np.mean(test_labels_280))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "testings=pd.read_csv(\"test_data.csv\")\n",
    "colnames2=list(testings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable: SameEntryTRUE        Importance: 0.0\n",
    "# Variable: CityBoston           Importance: 0.0\n",
    "# Variable: CityChicago          Importance: 0.0\n",
    "# Variable: CityPhiladelphia     Importance: 0.0\n",
    "# Variable: RushHour:SameEntryTRUE Importance: 0.0\n",
    "# Variable: Weekend:CityBoston   Importance: 0.0\n",
    "# Variable: Weekend:CityChicago  Importance: 0.0\n",
    "# Variable: RushHour:CityBoston  Importance: 0.0\n",
    "# Variable: RushHour:CityChicago Importance: 0.0\n",
    "# Variable: RushHour:CityPhiladelphia Importance: 0.0\n",
    "# Variable: ExitHeadingNE:RushHour Importance: 0.0\n",
    "# colnames[:20]+[colnames[39]]+colnames[41:44]+colnames[47:]\n",
    "testing_features=testings[colnames_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_120=rf.predict(testing_features)\n",
    "predict_150=rf_150.predict(testing_features)\n",
    "predict_180=rf_180.predict(testing_features)\n",
    "predict_220=rf_220.predict(testing_features)\n",
    "predict_250=rf_250.predict(testing_features)\n",
    "predict_280=rf_280.predict(testing_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"predict_120.csv\",predict_120,delimiter=\",\")\n",
    "np.savetxt(\"predict_150.csv\",predict_150,delimiter=\",\")\n",
    "np.savetxt(\"predict_180.csv\",predict_180,delimiter=\",\")\n",
    "np.savetxt(\"predict_220.csv\",predict_220,delimiter=\",\")\n",
    "np.savetxt(\"predict_250.csv\",predict_250,delimiter=\",\")\n",
    "np.savetxt(\"predict_280.csv\",predict_280,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data into xgb format\n",
    "D_train = xgb.DMatrix(train_features2, label=train_labels2)\n",
    "D_test = xgb.DMatrix(test_features2, label=test_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Set-ups\n",
    "param = {\n",
    "    'eta': 0.01, \n",
    "    'max_depth': 10,  \n",
    "    'objective': 'reg:linear'} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:52:33] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=800,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First\n",
    "from xgboost import XGBRegressor\n",
    "model=XGBRegressor(max_depth=8,n_estimators=800)\n",
    "model.fit(train_features2,train_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "preds = model.predict(test_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.12837845627995847\n"
     ]
    }
   ],
   "source": [
    "errors = sum((preds - test_labels2)**2)\n",
    "SST=sum((test_labels2-np.mean(test_labels2))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:20:49] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second\n",
    "model_150=XGBRegressor(max_depth=10,n_estimators=300)\n",
    "model_150.fit(train_features_150,train_labels_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.5099024828785143\n"
     ]
    }
   ],
   "source": [
    "preds = model_150.predict(test_features_150)\n",
    "errors = sum((preds - test_labels_150)**2)\n",
    "SST=sum((test_labels_150-np.mean(test_labels_150))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:51:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third\n",
    "model_180=XGBRegressor(max_depth=10,n_estimators=300)\n",
    "model_180.fit(train_features_180,train_labels_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.5458434748675767\n"
     ]
    }
   ],
   "source": [
    "preds = model_180.predict(test_features_180)\n",
    "errors = sum((preds - test_labels_180)**2)\n",
    "SST=sum((test_labels_180-np.mean(test_labels_180))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:59:51] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First\n",
    "model_220=XGBRegressor(max_depth=10,n_estimators=300)\n",
    "model_220.fit(train_features_220,train_labels_220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.27494830889977895\n"
     ]
    }
   ],
   "source": [
    "preds = model_220.predict(test_features_220)\n",
    "errors = sum((preds - test_labels_220)**2)\n",
    "SST=sum((test_labels_220-np.mean(test_labels_220))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:09:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second\n",
    "model_250=XGBRegressor(max_depth=10,n_estimators=300)\n",
    "model_250.fit(train_features_250,train_labels_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.4996875274390705\n"
     ]
    }
   ],
   "source": [
    "preds = model_250.predict(test_features_250)\n",
    "errors = sum((preds - test_labels_250)**2)\n",
    "SST=sum((test_labels_250-np.mean(test_labels_250))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:19:59] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third\n",
    "model_280=XGBRegressor(max_depth=10,n_estimators=300)\n",
    "model_280.fit(train_features_280,train_labels_280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6834126230302633\n"
     ]
    }
   ],
   "source": [
    "preds = model_280.predict(test_features_280)\n",
    "errors = sum((preds - test_labels_280)**2)\n",
    "SST=sum((test_labels_280-np.mean(test_labels_280))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testings=pd.read_csv(\"test_data.csv\")\n",
    "testing_features=testings[colnames_2]\n",
    "testing_features=np.array(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_120_2=model.predict(testing_features)\n",
    "predict_220_2=model_220.predict(testing_features)\n",
    "predict_250_2=model_250.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Latitude', 'Longitude', 'EntryHeadingN', 'EntryHeadingNE',\n",
       "       'EntryHeadingNW', 'EntryHeadingS', 'EntryHeadingSE', 'EntryHeadingSW',\n",
       "       'EntryHeadingW', 'ExitHeadingN', 'ExitHeadingNE', 'ExitHeadingNW',\n",
       "       'ExitHeadingS', 'ExitHeadingSE', 'ExitHeadingSW', 'ExitHeadingW',\n",
       "       'RushHour', 'Weekend', 'RushMonth', 'SameStreetTRUE',\n",
       "       'RushHour:Weekend', 'Weekend:CityBoston', 'Weekend:CityChicago',\n",
       "       'Weekend:CityPhiladelphia', 'RushMonth:CityBoston',\n",
       "       'RushMonth:CityChicago', 'RushMonth:CityPhiladelphia',\n",
       "       'RushMonth:SameEntryTRUE', 'ExitHeadingN:RushHour',\n",
       "       'ExitHeadingNE:RushHour', 'ExitHeadingNW:RushHour',\n",
       "       'ExitHeadingS:RushHour', 'ExitHeadingSE:RushHour',\n",
       "       'ExitHeadingSW:RushHour', 'ExitHeadingW:RushHour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"predict_120.csv\",predict_120_2,delimiter=\",\")\n",
    "np.savetxt(\"predict_220.csv\",predict_220_2,delimiter=\",\")\n",
    "np.savetxt(\"predict_250.csv\",predict_250_2,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.637167, 12.282653, 20.660704, 12.282653, 20.660704, 42.201572,\n",
       "       42.201572, 20.660704, 17.637167, 63.738644, 23.376532, 12.740761,\n",
       "       63.738644, 28.425093, 23.376532, 30.73248 , 53.50174 , 63.738644,\n",
       "       23.376532, 30.73248 , 12.740761, 53.50174 , 30.73248 , 63.738644,\n",
       "       23.376532, 12.740761, 53.50174 , 63.738644, 23.376532, 30.73248 ,\n",
       "       12.740761, 63.738644, 23.376532, 12.740761, 23.376532, 53.50174 ,\n",
       "       23.376532, 30.73248 , 12.740761, 53.50174 , 23.376532, 12.740761,\n",
       "       53.50174 , 23.376532, 53.50174 , 30.73248 , 12.740761, 53.50174 ,\n",
       "       23.376532, 30.73248 , 12.740761, 20.660704, 17.637167, 12.282653,\n",
       "       12.282653, 42.201572, 42.201572, 20.660704, 17.637167, 12.282653],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightBGM\n",
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(train_features2, label=train_labels2)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.3\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['max_depth'] = 10\n",
    "params['n_estimators'] = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "clf = lgb.train(params, d_train)\n",
    "y_pred=clf.predict(test_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "rf_120=pickle.load(open(\"rf_120.sav\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_contrast=rf_120.predict(test_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_predict_contrast=np.array(y_predict_contrast)\n",
    "#y_predict_contrast[y_predict_contrast<=0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.2913942717309447\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_predict_contrast - test_labels2)**2)\n",
    "SST=sum((test_labels2-np.mean(test_labels2))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "## Second\n",
    "d_train = lgb.Dataset(train_features_150, label=train_labels_150)\n",
    "clf_150 = lgb.train(params, d_train)\n",
    "#Prediction\n",
    "y_pred=clf_150.predict(test_features_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.5323388621760561\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_pred - test_labels_150)**2)\n",
    "SST=sum((test_labels_150-np.mean(test_labels_150))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third\n",
    "d_train = lgb.Dataset(train_features_180, label=train_labels_180)\n",
    "clf_180 = lgb.train(params, d_train)\n",
    "#Prediction\n",
    "y_pred=clf_180.predict(test_features_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.5805417464713567\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_pred - test_labels_180)**2)\n",
    "SST=sum((test_labels_180-np.mean(test_labels_180))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First\n",
    "d_train = lgb.Dataset(train_features_220, label=train_labels_220)\n",
    "clf_220 = lgb.train(params, d_train)\n",
    "#Prediction\n",
    "y_pred=clf_220.predict(test_features_220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.2728844627041359\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_pred - test_labels_220)**2)\n",
    "SST=sum((test_labels_220-np.mean(test_labels_220))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second\n",
    "d_train = lgb.Dataset(train_features_250, label=train_labels_250)\n",
    "clf_250 = lgb.train(params, d_train)\n",
    "#Prediction\n",
    "y_pred=clf_250.predict(test_features_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.5121416204004192\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_pred - test_labels_250)**2)\n",
    "SST=sum((test_labels_250-np.mean(test_labels_250))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third\n",
    "d_train = lgb.Dataset(train_features_280, label=train_labels_280)\n",
    "clf_280 = lgb.train(params, d_train)\n",
    "#Prediction\n",
    "y_pred=clf_280.predict(test_features_280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.6865774362763819\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_pred - test_labels_280)**2)\n",
    "SST=sum((test_labels_280-np.mean(test_labels_280))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_80 = np.array(features['TimeFromFirstStop_p80'])\n",
    "train_features_80, test_features_80, train_labels_80, test_labels_80 = train_test_split(features_2, labels_80, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['learning_rate'] = 0.3\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['max_depth'] = 10\n",
    "params['n_estimators'] = 2000\n",
    "d_train = lgb.Dataset(np.array(train_features_80), label=np.array(train_labels_80))\n",
    "clf_80 = lgb.train(params, d_train)\n",
    "#Prediction\n",
    "y_pred=clf_80.predict(test_features_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.999996320303062\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_pred - test_labels_80)**2)\n",
    "SST=sum((test_labels_80-np.mean(test_labels_80))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_60 = np.array(features['TimeFromFirstStop_p60'])\n",
    "train_features_60, test_features_60, train_labels_60, test_labels_60 = train_test_split(features_2, labels_60, test_size = 0.2, random_state = 42)\n",
    "d_train = lgb.Dataset(np.array(train_features_60), label=np.array(train_labels_60))\n",
    "clf_60 = lgb.train(params, d_train)\n",
    "#Prediction\n",
    "y_pred=clf_60.predict(test_features_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.39565347025096587\n"
     ]
    }
   ],
   "source": [
    "errors = sum((y_pred - test_labels_80)**2)\n",
    "SST=sum((test_labels_80-np.mean(test_labels_80))**2)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('R^2:',1-errors/SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train p_80 ### fit with predicted p_80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
